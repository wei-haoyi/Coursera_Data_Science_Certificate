View(bigram)
# Prepare the data for the quiz.
# load packages ----
library(corpus) # corpus is an R text processing package with full suppport for international text (Unicode)
library(dplyr)
library(ggplot2)
library(tm) # package used to remove numbers and strip whitespace from a text document
library(stringi) # the function checks whether all bytes in a string are in the ascii set 1,2,3,4...,127
# Set working directory
setwd("~/Documents/GitHub/Coursera_Data_Science_Certificate/10_Data Science Capstone")
# 1. Download and Read the data from web (Date: Dec 28, 2022) ----
# url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
# download.file(url, "final_dataset.zip")
# unzip("final_dataset.zip")
# 2. Combine and subset the data ----
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding="UTF-8", skipNul=TRUE)
data_combine <- c(twitter, blogs, news)
# data_sample <- data_combine
data_sample <- sample(data_combine, size=100000, replace=FALSE)
# 3. Clean the dataset ----
# remove special characters
data_sample_temp1 <- iconv(data_sample, "UTF-8","ASCII",sub="")
# Remove numbers from text document
data_sample_temp2 <- removeNumbers(data_sample_temp1)
# Remove white spaces from text document
data_sample_temp3 <- stripWhitespace(data_sample_temp2)
# Set the entire dataset to lower case
data_sample_temp4 <- tolower(data_sample_temp3)
# remove leading and tailing white space
data_sample_temp5 <- trimws(data_sample_temp4)
# Profanity filtering - task 1, remove bad words.
# download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt", destfile = "bad_words.txt")
badword <- readLines("bad_words.txt", encoding="UTF-8")
data_sample_clean <- removeWords(data_sample_temp5, c(badword))
# 4. Get unigram, bigram, trigram ----
unigram <- term_stats(data_sample_clean, drop=stopwords_en, drop_punct = TRUE, ngrams=1,type=T) # remove punctuation and english stop words (is, this, and....)
save(unigram, file = "unigram.Rdata")
bigram <- term_stats(data_sample_clean, drop=stopwords_en, drop_punct = TRUE, ngrams=2,type=T)
save(bigram, file = "bigram.Rdata")
trigram <- term_stats(data_sample_clean, drop=stopwords_en, drop_punct = TRUE, ngrams=3,type=T)
save(trigram, file = "trigram.Rdata")
quogram <- term_stats(data_sample_clean, drop=stopwords_en, drop_punct = TRUE, ngrams=4,type=T)
save(quogram, file = "quogram.Rdata")
View(quogram)
~/Downloads/capstone_swiftkey-master/shiny/prediction.R
library(readr)
guess_encoding(data_combine, n_max = 1000)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE), ngrams=1,type=T) # remove punctuation and english stop words (is, this, and....)
save(unigram, file = "unigram.Rdata")
bigram <- term_stats(data_sample_clean,  text_filter(drop=stopwords_en, drop_symbol = TRUE), ngrams=2,type=T)
save(bigram, file = "bigram.Rdata")
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE), ngrams=3,type=T)
save(trigram, file = "trigram.Rdata")
View(trigram)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE), ngrams=3,type=T)
View(trigram)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE), ngrams=3,type=T)
View(trigram)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE), ngrams=1,type=T) # remove punctuation and english stop words (is, this, and....)
save(unigram, file = "unigram.Rdata")
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE), ngrams=2,type=T)
save(bigram, file = "bigram.Rdata")
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE), ngrams=3,type=T)
save(trigram, file = "trigram.Rdata")
x <- "wo  xiang    ni le "
text_filter(x,remove_ignorable = TRUE)
xx <- term_stats(x, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T))
xx <- term_stats(x, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T)
View(xx)
xx <- term_stats(x, text_filter(drop=stopwords_en,  remove_ignorable=F,drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T)
xx <- term_stats(x, text_filter(remove_ignorable=FALSE), ngrams=1,type=T)
x <- "WO AI NI"
xx <- term_stats(x, text_filter(remove_ignorable=FALSE), ngrams=1,type=T)
# 4. Get unigram, bigram, trigram ----
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
save(unigram, file = "unigram.Rdata")
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T)
save(bigram, file = "bigram.Rdata")
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T)
save(trigram, file = "trigram.Rdata")
# Prepare the data for the quiz.
# load packages ----
library(corpus) # corpus is an R text processing package with full suppport for international text (Unicode)
library(dplyr)
library(ggplot2)
library(tm) # package used to remove numbers and strip whitespace from a text document
library(stringi) # the function checks whether all bytes in a string are in the ascii set 1,2,3,4...,127
# Set working directory
setwd("~/Documents/GitHub/Coursera_Data_Science_Certificate/10_Data Science Capstone/shiny")
# 1. Download and Read the data from web (Date: Dec 28, 2022) ----
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(url, "final_dataset.zip")
unzip("final_dataset.zip")
# 2. Combine and subset the data ----
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding="UTF-8", skipNul=TRUE)
data_combine <- c(twitter, blogs, news)
# check the encoding of of data_combine
# library(readr)
# guess_encoding(data_combine, n_max = 1000)
# data_sample <- data_combine
data_sample <- sample(data_combine, size=100000, replace=FALSE)
# 3. Clean the dataset ----
# remove special characters
data_sample_temp1 <- iconv(data_sample, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt", destfile = "bad_words.txt")
badword <- readLines("bad_words.txt", encoding="UTF-8")
data_sample_clean <- removeWords(data_sample_temp1, c(badword))
# 4. Get unigram, bigram, trigram ----
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
save(unigram, file = "unigram.Rdata")
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T)
save(bigram, file = "bigram.Rdata")
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T)
save(trigram, file = "trigram.Rdata")
View(bigram)
View(unigram)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="")
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(type=NextWord)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord==type)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord==type1)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(type1=NextWord)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
selcet(-support)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
select(-support)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
select(-support)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T)
View(bigram)
View(bigram)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term) %>%
rename(term=type1)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term,-support) %>%
rename(term=type1)
View(trigram)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
mutate(term1 = paste(type1,type2))
trigram$term1[1]
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
order(term) %>%
rename(NextWord=type3)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term,-support) %>%
rename(term=type1) %>%
rename(NextWord=type2)
View(bigram)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
order(term) %>%
rename(NextWord=type3)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
arrange(term) %>%
rename(NextWord=type3)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
rellocate(term) %>%
rename(NextWord=type3)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3) %>%
arrange(term)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3) %>%
mutate(count=count*10000)
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
select(-support)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term,-support) %>%
rename(term=type1) %>%
rename(NextWord=type2)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3) %>%
mutate(count=count*10000)
final <- rbind(unigram,bigram,trigram)
View(final)
x <= "wo ai ni > < haha $ @"
x <- "wo ai ni > < haha $ @"
text_filter(x,drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T)
text_filter(x,drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE)
d2 <- data.frame(x, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE))
summary(final$count[final$count<10000],)
sen <- "wo ai ni $ sb fuck @"
bian <- term_stats(sen, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T)
View(bian)
View(final)
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final <- trimws(final)
final[is.na(final)] <- ""
return(final)
}
xxx <- fun.input("")
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final$term <- trimws(final$term)
final[is.na(final)] <- ""
return(final)
}
xxx <- fun.input("")
View(xxx)
xxx <- fun.input("cao ni ma")
xxx <- fun.input("")
xxx$term[3]
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final$term <- trimws(final$term)
final[is.na(final)] <- ""
final[term=="NA"] <- ""
return(final)
}
xxx <- fun.input("")
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final$term <- trimws(final$term)
final[is.na(final)] <- ""
final[term=="NA",] <- ""
return(final)
}
xxx <- fun.input("")
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final$term <- trimws(final$term)
final[is.na(final)] <- ""
final[final$term=="NA",] <- ""
return(final)
}
xxx <- fun.input("")
View(xxx)
xxx
xxx$term[1:3]
View(final)
# Prepare the data for the quiz.
# load packages ----
library(corpus) # corpus is an R text processing package with full suppport for international text (Unicode)
library(dplyr)
library(ggplot2)
library(tm) # package used to remove numbers and strip whitespace from a text document
library(stringi) # the function checks whether all bytes in a string are in the ascii set 1,2,3,4...,127
# Set working directory
setwd("~/Documents/GitHub/Coursera_Data_Science_Certificate/10_Data Science Capstone/shiny")
# 1. Download and Read the data from web (Date: Dec 28, 2022) ----
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(url, "final_dataset.zip")
unzip("final_dataset.zip")
# 2. Combine and subset the data ----
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding="UTF-8", skipNul=TRUE)
data_combine <- c(twitter, blogs, news)
# check the encoding of of data_combine
# library(readr)
# guess_encoding(data_combine, n_max = 1000)
# data_sample <- data_combine
data_sample <- sample(data_combine, size=100000, replace=FALSE)
# 3. Clean the dataset ----
# remove special characters
data_sample_temp1 <- iconv(data_sample, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt", destfile = "bad_words.txt")
badword <- readLines("bad_words.txt", encoding="UTF-8")
data_sample_clean <- removeWords(data_sample_temp1, c(badword))
# 4. Get unigram, bigram, trigram ----
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
select(-support)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term,-support) %>%
rename(term=type1) %>%
rename(NextWord=type2) %>%
mutate(count=count*10000)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3) %>%
mutate(count=count*100000)
final <- rbind(unigram,bigram,trigram)
save(final, file = "final.Rdata")
load(final.Rdata)
load("final.Rdata")
View(final)
getwd()
setwd("/Users/wei/Documents/GitHub/Coursera_Data_Science_Certificate/10_Data Science Capstone/shiny")
fun.input <- function(sentence){
# Remove some symbols
# remove special characters
sentence1 <- iconv(sentence, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
badword <- readLines("bad_words.txt", encoding="UTF-8")
sentence1 <- removeWords(sentence1, c(badword))
# Remove white spaces from text document
sentence1 <- stripWhitespace(sentence1)
# Set the entire dataset to lower case
sentence1 <- tolower(sentence1)
# remove leading and tailing white space
sentence1 <- trimws(sentence1)
# make the sentence into a dataset
first <- word(sentence1, -1) # the last word
second <-word(sentence1, -2) # the last second last word
final <- data.frame(term=c(NA,first,paste(second,first)))
final$term <- trimws(final$term)
final[is.na(final)] <- ""
final[final$term=="NA",] <- ""
return(final)
}
fun.predict = function(x, n =100) {
sentence <- fun.input(x)
combo <- inner_join(sentence,final,by="term") %>%
arrange(count) %>%
select(NextWord,count)
return(combo[1:n,])
}
v <- fun.predict("I love", n=100)
View(v)
# Prepare the data for the quiz.
# load packages ----
library(corpus) # corpus is an R text processing package with full suppport for international text (Unicode)
library(dplyr)
library(ggplot2)
library(tm) # package used to remove numbers and strip whitespace from a text document
library(stringi) # the function checks whether all bytes in a string are in the ascii set 1,2,3,4...,127
# Set working directory
setwd("~/Documents/GitHub/Coursera_Data_Science_Certificate/10_Data Science Capstone/shiny")
# 1. Download and Read the data from web (Date: Dec 28, 2022) ----
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(url, "final_dataset.zip")
unzip("final_dataset.zip")
# 2. Combine and subset the data ----
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding="UTF-8", skipNul=TRUE)
data_combine <- c(twitter, blogs, news)
# check the encoding of of data_combine
# library(readr)
# guess_encoding(data_combine, n_max = 1000)
# data_sample <- data_combine
data_sample <- sample(data_combine, size=100000, replace=FALSE)
# 3. Clean the dataset ----
# remove special characters
data_sample_temp1 <- iconv(data_sample, "UTF-8","ASCII",sub="")
# Profanity filtering - task 1, remove bad words.
download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt", destfile = "bad_words.txt")
badword <- readLines("bad_words.txt", encoding="UTF-8")
data_sample_clean <- removeWords(data_sample_temp1, c(badword))
# 4. Get unigram, bigram, trigram ----
unigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=1,type=T) %>% # remove punctuation and english stop words (is, this, and....),remove number, remove space, lowercase
mutate(term="") %>%
rename(NextWord=type1) %>%
select(-support)
bigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=2,type=T) %>%
select(-term,-support) %>%
rename(term=type1) %>%
rename(NextWord=type2) %>%
mutate(count=count*10000)
trigram <- term_stats(data_sample_clean, text_filter(drop=stopwords_en, drop_symbol = TRUE, drop_punct = TRUE, drop_number = TRUE), ngrams=3,type=T) %>%
select(-term,-support) %>%
mutate(term = paste(type1,type2)) %>%
select(-type1,-type2) %>%
relocate(term) %>%
rename(NextWord=type3) %>%
mutate(count=count*100000)
final <- rbind(unigram,bigram,trigram)
save(final, file = "final.Rdata")
View(final)
fun.predict = function(x, n =100) {
sentence <- fun.input(x)
combo <- inner_join(sentence,final,by="term") %>%
arrange(-count) %>%
select(NextWord,count)
return(combo[1:n,])
}
v <- fun.predict("I love", n=100)
View(v)
runApp()
